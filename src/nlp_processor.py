import nltk
import spacy
import re
import pandas as pd
from typing import List, Dict, Tuple, Set
from fuzzywuzzy import fuzz, process
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from unidecode import unidecode

class NLPProcessor:
    def __init__(self):
        """Kh·ªüi t·∫°o b·ªô x·ª≠ l√Ω NLP v·ªõi h·ªó tr·ª£ ti·∫øng Vi·ªát m·∫°nh"""
        self.setup_nlp()
        self.load_food_keywords()
        self.setup_vietnamese_stopwords()
    
    def setup_nlp(self):
        """C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán NLP"""
        try:
            # Download NLTK data
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
            
            # Load spaCy model
            try:
                self.nlp = spacy.load("en_core_web_sm")
            except:
                print("Kh√¥ng th·ªÉ t·∫£i spaCy model, s·ª≠ d·ª•ng ch·∫ø ƒë·ªô c∆° b·∫£n")
                self.nlp = None
        except Exception as e:
            print(f"L·ªói khi c√†i ƒë·∫∑t NLP: {e}")
            self.nlp = None
    
    def setup_vietnamese_stopwords(self):
        """Thi·∫øt l·∫≠p t·ª´ d·ª´ng ti·∫øng Vi·ªát"""
        self.vietnamese_stopwords = {
            't√¥i', 'toi', 'm√¨nh', 'minh', 'em', 'anh', 'ch·ªã', 'chi', 'c·ªßa', 'cua',
            'v√†', 'va', 'v·ªõi', 'voi', 'cho', 'ƒë·ªÉ', 'de', 't·ª´', 'tu', 'trong', 'ngo√†i',
            'ngoai', 'tr√™n', 'tren', 'd∆∞·ªõi', 'duoi', 'n√†y', 'nay', 'ƒë√≥', 'do',
            'r·∫•t', 'rat', 'l·∫Øm', 'lam', 'nhi·ªÅu', 'nhieu', '√≠t', 'it', 'm·ªôt', 'mot',
            'c√≥', 'co', 'kh√¥ng', 'khong', 'ƒë∆∞·ª£c', 'duoc', 'l√†', 'la', 's·∫Ω', 'se',
            'ƒë√£', 'da', 'ƒëang', 'dang', 'b·ªã', 'bi', 'ƒë∆∞·ª£c', 'duoc', 'hay', 'ho·∫∑c', 'hoac'
        }
    
    def load_food_keywords(self):
        """T·∫£i t·ª´ kh√≥a th·ª±c ph·∫©m v√† thu·ªôc t√≠nh v·ªõi h·ªó tr·ª£ ti·∫øng Vi·ªát m·∫°nh"""
        
        # ·∫®m th·ª±c theo v√πng mi·ªÅn
        self.cuisine_keywords = {
            'vietnamese': [
                'vietnamese', 'vietnam', 'vi·ªát nam', 'viet nam', 'vi·ªát', 'viet',
                'pho', 'ph·ªü', 'bun', 'b√∫n', 'banh', 'b√°nh', 'che', 'ch√®', 'goi', 'g·ªèi',
                'nem', 'spring roll', 'ch·∫£ c√°', 'cha ca', 'b√∫n b√≤', 'bun bo',
                'c∆°m t·∫•m', 'com tam', 'b√°nh m√¨', 'banh mi', 'b√°nh x√®o', 'banh xeo',
                'cao l·∫ßu', 'cao lau', 'm√¨ qu·∫£ng', 'mi quang', 'h·ªß ti·∫øu', 'hu tieu',
                'b√∫n ri√™u', 'bun rieu', 'b√°nh cu·ªën', 'banh cuon', 'ch·∫£ c√° l√£ v·ªçng',
                'b√∫n ch·∫£', 'bun cha', 'ph·ªü g√†', 'pho ga', 'ph·ªü b√≤', 'pho bo',
                'ch√°o', 'chao', 'x√¥i', 'xoi', 'b√°nh ch∆∞ng', 'banh chung'
            ],
            'chinese': [
                'chinese', 'china', 'trung qu·ªëc', 'trung quoc', 't√†u', 'tau',
                'dim sum', 'dumpling', 'h√° c·∫£o', 'ha cao', 's·ªßi c·∫£o', 'sui cao',
                'm√¨ x√†o', 'mi xao', 'c∆°m chi√™n', 'com chien', 'th·ªãt x√° x√≠u', 'thit xa xiu',
                'kung pao', 'mapo tofu', 'peking duck', 'v·ªãt quay', 'vit quay',
                'l·∫©u t·ª© xuy√™n', 'lau tu xuyen', 'm√≥n t√†u', 'mon tau', 'dimsum'
            ],
            'italian': [
                'italian', 'italy', '√Ω', 'italia', 'pasta', 'pizza', 'spaghetti',
                'lasagna', 'carbonara', 'bolognese', 'risotto', 'ravioli',
                'gnocchi', 'tiramisu', 'gelato', 'bruschetta', 'focaccia',
                'm√¨ √Ω', 'mi y', 'b√°nh pizza', 'banh pizza'
            ],
            'mexican': [
                'mexican', 'mexico', 'taco', 'burrito', 'quesadilla', 'salsa',
                'guacamole', 'nachos', 'enchilada', 'fajita', 'churro',
                'mexico', 'mex', 'm√≥n m√™-hi-c√¥', 'mon me-hi-co'
            ],
            'indian': [
                'indian', 'india', '·∫•n ƒë·ªô', 'an do', 'curry', 'c√† ri', 'ca ri',
                'biryani', 'tandoori', 'masala', 'naan', 'samosa', 'dal',
                'tikka', 'vindaloo', 'korma', 'rogan josh', 'lassi'
            ],
            'japanese': [
                'japanese', 'japan', 'nh·∫≠t b·∫£n', 'nhat ban', 'nh·∫≠t', 'nhat',
                'sushi', 'sashimi', 'ramen', 'tempura', 'miso', 'udon', 'soba',
                'yakitori', 'tonkatsu', 'okonomiyaki', 'takoyaki', 'bento',
                'mochi', 'wasabi', 'teriyaki', 'katsu', 'gyoza'
            ],
            'thai': [
                'thai', 'thailand', 'th√°i lan', 'thai lan', 'th√°i', 'thai',
                'tom yum', 'pad thai', 'green curry', 'som tam', 'massaman',
                'pad see ew', 'larb', 'sticky rice', 'mango sticky rice',
                'tom kha', 'red curry', 'panang', 'thai basil'
            ],
            'korean': [
                'korean', 'korea', 'h√†n qu·ªëc', 'han quoc', 'h√†n', 'han',
                'kimchi', 'bulgogi', 'bibimbap', 'korean bbq', 'galbi',
                'japchae', 'tteokbokki', 'samgyeopsal', 'hotteok', 'banchan'
            ]
        }
        
        # S·ªü th√≠ch ƒÉn u·ªëng v√† ch·∫ø ƒë·ªô ƒÉn
        self.dietary_keywords = {
            'vegetarian': [
                'vegetarian', 'veggie', 'plant based', 'no meat', 'chay', 'ƒÉn chay', 'an chay',
                'kh√¥ng th·ªãt', 'khong thit', 'thu·∫ßn chay', 'thuan chay', 'chay tr∆∞·ªùng',
                'chay truong', 'kh√¥ng ƒë·ªông v·∫≠t', 'khong dong vat', 'rau c·ªß', 'rau cu'
            ],
            'vegan': [
                'vegan', 'plant only', 'dairy free', 'no animal', 'thu·∫ßn chay', 'thuan chay',
                'ho√†n to√†n chay', 'hoan toan chay', 'kh√¥ng s·ªØa', 'khong sua',
                'kh√¥ng tr·ª©ng', 'khong trung', 'plant-based'
            ],
            'low_calorie': [
                'low calorie', 'diet', 'light', 'healthy', 'low cal', '√≠t calo', 'it calo',
                'gi·∫£m c√¢n', 'giam can', 'ƒÉn ki√™ng', 'an kieng', 'healthy', 'l√†nh m·∫°nh',
                'lanh manh', '√≠t b√©o', 'it beo', 'low fat', 'fitness'
            ],
            'high_protein': [
                'high protein', 'protein rich', 'muscle', 'gym', 'nhi·ªÅu protein', 'nhieu protein',
                'ƒë·∫°m cao', 'dam cao', 'tƒÉng c∆°', 'tang co', 'bodybuilding',
                'protein tinggi', 'nhi·ªÅu ƒë·∫°m', 'nhieu dam'
            ],
            'gluten_free': [
                'gluten free', 'no gluten', 'kh√¥ng gluten', 'khong gluten',
                'kh√¥ng ch·ª©a gluten', 'khong chua gluten', 'celiac', 'wheat free'
            ],
            'diabetic': [
                'diabetic', 'diabetes', 'ti·ªÉu ƒë∆∞·ªùng', 'tieu duong', 'ƒë√°i th√°o ƒë∆∞·ªùng',
                'dai thao duong', '√≠t ƒë∆∞·ªùng', 'it duong', 'no sugar', 'sugar free'
            ]
        }
        
        # V·ªã gi√°c v√† ƒë·ªô cay
        self.taste_keywords = {
            'spicy': [
                'spicy', 'hot', 'chili', 'pepper', 'fire', 'cay', 'c·∫•y', 'cap',
                '·ªõt', 'ot', 'Îß§Ïö¥', 'cay n·ªìng', 'cay nong', 'c·ª±c cay', 'cuc cay',
                'si√™u cay', 'sieu cay', 't√™ l∆∞·ª°i', 'te luoi', 'cay nh∆∞ l·ª≠a',
                'cay nhu lua', 'ch√°y mi·ªáng', 'chay mieng'
            ],
            'sweet': [
                'sweet', 'dessert', 'sugar', 'cake', 'candy', 'ng·ªçt', 'ngot',
                'ƒë∆∞·ªùng', 'duong', 'b√°nh ng·ªçt', 'banh ngot', 'tr√°ng mi·ªáng', 'trang mieng',
                'ch√®', 'che', 'kem', 'b√°nh', 'banh', 'k·∫πo', 'keo', 'm·∫≠t', 'mat'
            ],
            'sour': [
                'sour', 'acid', 'lemon', 'lime', 'vinegar', 'chua', 'chua',
                'chanh', 'gi·∫•m', 'giam', 'me', 'm·∫ª', 'chua cay', 'chua ng·ªçt',
                'chua ngot', 'tamarind'
            ],
            'salty': [
                'salty', 'salt', 'savory', 'umami', 'm·∫∑n', 'man', 'mu·ªëi', 'muoi',
                'Ïß†', 'm·∫∑n m√†', 'man ma', 'ƒë·∫≠m ƒë√†', 'dam da', 'm·∫∑n ng·ªçt', 'man ngot'
            ],
            'bitter': [
                'bitter', 'ƒë·∫Øng', 'dang', 'kh·ªï qua', 'kho qua', 'bitter melon',
                'coffee', 'c√† ph√™', 'ca phe', 'dark chocolate'
            ],
            'mild': [
                'mild', 'nh·∫π', 'nhe', 'thanh ƒë·∫°m', 'thanh dam', 'kh√¥ng cay', 'khong cay',
                'd·ªãu', 'diu', 'nh·∫°t', 'nhat', 't∆∞∆°i m√°t', 'tuoi mat'
            ]
        }
        
        # Nguy√™n li·ªáu chi ti·∫øt
        self.ingredient_keywords = {
            'chicken': [
                'chicken', 'poultry', 'hen', 'g√†', 'ga', 'th·ªãt g√†', 'thit ga',
                'g√† ta', 'ga ta', 'g√† c√¥ng nghi·ªáp', 'ga cong nghiep', '·ª©c g√†', 'uc ga',
                'ƒë√πi g√†', 'dui ga', 'c√°nh g√†', 'canh ga', 'g√† r√°n', 'ga ran'
            ],
            'beef': [
                'beef', 'cow', 'steak', 'ground beef', 'b√≤', 'bo', 'th·ªãt b√≤', 'thit bo',
                'b√≤ t√°i', 'bo tai', 'b√≤ ch√≠n', 'bo chin', 'b√≤ vi√™n', 'bo vien',
                'thƒÉn b√≤', 'than bo', 's∆∞·ªùn b√≤', 'suon bo', 'b√≤ kho', 'bo kho'
            ],
            'pork': [
                'pork', 'pig', 'bacon', 'ham', 'heo', 'l·ª£n', 'lon', 'th·ªãt heo', 'thit heo',
                'th·ªãt l·ª£n', 'thit lon', 'ba ch·ªâ', 'ba chi', 's∆∞·ªùn heo', 'suon heo',
                'ch√¢n gi√≤', 'chan gio', 'th·ªãt x√° x√≠u', 'thit xa xiu'
            ],
            'fish': [
                'fish', 'salmon', 'tuna', 'cod', 'seafood', 'c√°', 'ca', 'c√° h·ªìi', 'ca hoi',
                'c√° ng·ª´', 'ca ngu', 'c√° thu', 'ca thu', 'c√° ch√©p', 'ca chep',
                'c√° r√¥', 'ca ro', 'c√° di√™u h·ªìng', 'ca dieu hong', 'c√° basa', 'ca basa'
            ],
            'shrimp': [
                'shrimp', 'prawn', 'lobster', 'crab', 't√¥m', 'tom', 't√¥m c√†ng', 'tom cang',
                't√¥m s√∫', 'tom su', 'cua', 'c√†o c√†o', 'cao cao', 't√¥m th·∫ª', 'tom the',
                't√¥m t√≠t', 'tom tit', 'ngh√™u', 'ngheu', 's√≤', 'so'
            ],
            'vegetables': [
                'vegetables', 'veggie', 'carrot', 'broccoli', 'spinach', 'rau', 'rau c·ªß', 'rau cu',
                'c√† r√≥t', 'ca rot', 's√∫p l∆°', 'sup lo', 'rau bina', 'c·∫£i b√≥ x√¥i', 'cai bo xoi',
                'c·∫£i th·∫£o', 'cai thao', 'rau mu·ªëng', 'rau muong', 'rau lang', 'ƒëu ƒë·ªß xanh',
                'du du xanh', 'b·∫Øp c·∫£i', 'bap cai', 'c√† chua', 'ca chua', 'd∆∞a chu·ªôt', 'dua chuot'
            ],
            'rice': [
                'rice', 'grain', 'jasmine rice', 'brown rice', 'c∆°m', 'com', 'g·∫°o', 'gao',
                'c∆°m tr·∫Øng', 'com trang', 'c∆°m t·∫•m', 'com tam', 'c∆°m d·∫ªo', 'com deo',
                'g·∫°o t·∫ª', 'gao te', 'g·∫°o n√†ng h∆∞∆°ng', 'gao nang huong', 'g·∫°o ST25'
            ],
            'noodles': [
                'noodles', 'pasta', 'spaghetti', 'ramen', 'b√∫n', 'bun', 'm√¨', 'mi',
                'ph·ªü', 'pho', 'mi·∫øn', 'mien', 'b√°nh canh', 'banh canh', 'b√°nh ph·ªü', 'banh pho',
                'h·ªß ti·∫øu', 'hu tieu', 'm√¨ g√≥i', 'mi goi', 'm√¨ t√¥m', 'mi tom'
            ],
            'egg': [
                'egg', 'eggs', 'omelet', 'scrambled', 'tr·ª©ng', 'trung', 'tr·ª©ng g√†', 'trung ga',
                'tr·ª©ng v·ªãt', 'trung vit', 'tr·ª©ng c√∫t', 'trung cut', 'tr·ª©ng chi√™n', 'trung chien',
                'tr·ª©ng lu·ªôc', 'trung luoc', 'tr·ª©ng ·ªëp la', 'trung op la'
            ],
            'tofu': [
                'tofu', 'soy', 'ƒë·∫≠u h≈©', 'dau hu', 'ƒë·∫≠u ph·ª•', 'dau phu', 't√†u h≈©', 'tau hu',
                'ƒë·∫≠u', 'dau', 'ƒë·∫≠u xanh', 'dau xanh', 'ƒë·∫≠u ƒë·ªè', 'dau do'
            ]
        }
        
        # Th·ªùi gian ƒÉn
        self.meal_time_keywords = {
            'breakfast': [
                'breakfast', 'morning', 'brunch', 'early', 's√°ng', 'sang', 'b·ªØa s√°ng', 'bua sang',
                'ƒëi·ªÉm t√¢m', 'diem tam', 'ƒÉn s√°ng', 'an sang', 's√°ng s·ªõm', 'sang som',
                'bu·ªïi s√°ng', 'buoi sang', 'b·ªØa ƒëi·ªÉm t√¢m', 'bua diem tam'
            ],
            'lunch': [
                'lunch', 'noon', 'midday', 'afternoon', 'tr∆∞a', 'trua', 'b·ªØa tr∆∞a', 'bua trua',
                'ƒÉn tr∆∞a', 'an trua', 'gi·ªØa tr∆∞a', 'giua trua', 'bu·ªïi tr∆∞a', 'buoi trua',
                'c∆°m tr∆∞a', 'com trua'
            ],
            'dinner': [
                'dinner', 'evening', 'night', 'supper', 't·ªëi', 'toi', 'b·ªØa t·ªëi', 'bua toi',
                'ƒÉn t·ªëi', 'an toi', 'bu·ªïi t·ªëi', 'buoi toi', 'c∆°m t·ªëi', 'com toi',
                'b·ªØa chi·ªÅu', 'bua chieu', 'chi·ªÅu', 'chieu'
            ],
            'snack': [
                'snack', 'light meal', 'quick bite', 'appetizer', 'ƒÉn v·∫∑t', 'an vat',
                'ƒë·ªì ƒÉn v·∫∑t', 'do an vat', 'nh√¢m nhi', 'nham nhi', 'ƒÉn ch∆°i', 'an choi',
                'b√°nh k·∫πo', 'banh keo', 'qu√† v·∫∑t', 'qua vat'
            ]
        }
        
        # Ph∆∞∆°ng ph√°p n·∫•u
        self.cooking_method_keywords = {
            'fried': ['chi√™n', 'chien', 'r√°n', 'ran', 'fried', 'deep fried', 'x√†o', 'xao'],
            'grilled': ['n∆∞·ªõng', 'nuong', 'grilled', 'bbq', 'barbecue', 'n∆∞·ªõng than', 'nuong than'],
            'boiled': ['lu·ªôc', 'luoc', 'boiled', 'lu·ªôc ch√≠n', 'luoc chin'],
            'steamed': ['h·∫•p', 'hap', 'steamed', 'h·∫•p ch√≠n', 'hap chin'],
            'soup': ['canh', 's√∫p', 'sup', 'soup', 'n∆∞·ªõc', 'nuoc', 'l·∫©u', 'lau'],
            'stir_fried': ['x√†o', 'xao', 'stir fried', 'rang', 'x√†o lƒÉn', 'xao lan'],
            'braised': ['kho', 'braised', 'rim', 'om', '·ªëm', 'ni√™u', 'nieu']
        }
        
        # C·ª≠a h√†ng v√† lo·∫°i h√¨nh
        self.restaurant_type_keywords = {
            'street_food': [
                'street food', 'food truck', 'roadside', 'ƒë∆∞·ªùng ph·ªë', 'duong pho',
                'qu√°n v·ªâa h√®', 'quan via he', 'ƒÉn v·∫∑t', 'an vat', 'h√†ng rong', 'hang rong'
            ],
            'restaurant': [
                'restaurant', 'nh√† h√†ng', 'nha hang', 'qu√°n ƒÉn', 'quan an',
                'fine dining', 'sang tr·ªçng', 'sang trong'
            ],
            'fast_food': [
                'fast food', 'quick', 'nhanh', 'th·ª©c ƒÉn nhanh', 'thuc an nhanh',
                'ƒë·ªì ƒÉn nhanh', 'do an nhanh'
            ],
            'home_cooking': [
                'home cooking', 'homemade', 'gia ƒë√¨nh', 'gia dinh', 'n·∫•u nh√†', 'nau nha',
                'c∆°m nh√†', 'com nha', 't·ª± n·∫•u', 'tu nau'
            ]
        }

    def normalize_vietnamese_text(self, text: str) -> str:
        """Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát"""
        if not isinstance(text, str):
            text = str(text)
        
        # Lo·∫°i b·ªè d·∫•u thanh
        normalized = unidecode(text.lower())
        
        # Thay th·∫ø c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát ti·∫øng Vi·ªát
        vietnamese_chars = {
            '√†': 'a', '√°': 'a', '·∫°': 'a', '·∫£': 'a', '√£': 'a', '√¢': 'a', '·∫ß': 'a', '·∫•': 'a',
            '·∫≠': 'a', '·∫©': 'a', '·∫´': 'a', 'ƒÉ': 'a', '·∫±': 'a', '·∫Ø': 'a', '·∫∑': 'a', '·∫≥': 'a', '·∫µ': 'a',
            '√®': 'e', '√©': 'e', '·∫π': 'e', '·∫ª': 'e', '·∫Ω': 'e', '√™': 'e', '·ªÅ': 'e', '·∫ø': 'e',
            '·ªá': 'e', '·ªÉ': 'e', '·ªÖ': 'e',
            '√¨': 'i', '√≠': 'i', '·ªã': 'i', '·ªâ': 'i', 'ƒ©': 'i',
            '√≤': 'o', '√≥': 'o', '·ªç': 'o', '·ªè': 'o', '√µ': 'o', '√¥': 'o', '·ªì': 'o', '·ªë': 'o',
            '·ªô': 'o', '·ªï': 'o', '·ªó': 'o', '∆°': 'o', '·ªù': 'o', '·ªõ': 'o', '·ª£': 'o', '·ªü': 'o', '·ª°': 'o',
            '√π': 'u', '√∫': 'u', '·ª•': 'u', '·ªß': 'u', '≈©': 'u', '∆∞': 'u', '·ª´': 'u', '·ª©': 'u',
            '·ª±': 'u', '·ª≠': 'u', '·ªØ': 'u',
            '·ª≥': 'y', '√Ω': 'y', '·ªµ': 'y', '·ª∑': 'y', '·ªπ': 'y',
            'ƒë': 'd', 'ƒê': 'd'
        }
        
        result = text.lower()
        for vn_char, replacement in vietnamese_chars.items():
            result = result.replace(vn_char, replacement)
        
        return result

    def extract_intent(self, user_input: str) -> Dict:
        """Tr√≠ch xu·∫•t √Ω ƒë·ªãnh t·ª´ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v·ªõi ƒë·ªô ch√≠nh x√°c cao"""
        if not isinstance(user_input, str):
            user_input = str(user_input)
        
        original_input = user_input.lower().strip()
        normalized_input = self.normalize_vietnamese_text(original_input)
        
        intent = {
            'cuisine': [],
            'dietary': [],
            'ingredients': [],
            'meal_time': [],
            'taste': [],
            'cooking_method': [],
            'restaurant_type': [],
            'raw_text': original_input,
            'normalized_text': normalized_input,
            'confidence': 0.0,
            'keywords_found': []
        }
        
        total_matches = 0
        found_keywords = []
        
        # H√†m helper ƒë·ªÉ t√¨m keywords
        def find_keywords_in_category(category_dict, category_name):
            nonlocal total_matches, found_keywords
            
            for item, keywords in category_dict.items():
                for keyword in keywords:
                    normalized_keyword = self.normalize_vietnamese_text(keyword)
                    
                    # Ki·ªÉm tra trong c·∫£ text g·ªëc v√† text chu·∫©n h√≥a
                    if (keyword in original_input or 
                        normalized_keyword in normalized_input or
                        keyword in normalized_input):
                        
                        if item not in intent[category_name]:
                            intent[category_name].append(item)
                            total_matches += 1
                            found_keywords.append(f"{category_name}:{item}:{keyword}")
                        break
        
        # T√¨m t·∫•t c·∫£ c√°c lo·∫°i keywords
        find_keywords_in_category(self.cuisine_keywords, 'cuisine')
        find_keywords_in_category(self.dietary_keywords, 'dietary')
        find_keywords_in_category(self.ingredient_keywords, 'ingredients')
        find_keywords_in_category(self.meal_time_keywords, 'meal_time')
        find_keywords_in_category(self.taste_keywords, 'taste')
        find_keywords_in_category(self.cooking_method_keywords, 'cooking_method')
        find_keywords_in_category(self.restaurant_type_keywords, 'restaurant_type')
        
        intent['keywords_found'] = found_keywords
        
        # T√≠nh confidence score n√¢ng cao
        word_count = len(original_input.split())
        base_confidence = min(total_matches / max(word_count, 1), 1.0)
        
        # Bonus cho vi·ªác t√¨m th·∫•y nhi·ªÅu lo·∫°i keywords
        category_bonus = len([cat for cat in ['cuisine', 'dietary', 'ingredients', 'meal_time', 'taste'] 
                             if intent[cat]]) * 0.1
        
        intent['confidence'] = min(base_confidence + category_bonus, 1.0)
        
        return intent

    def semantic_search(self, query: str, recipe_df: pd.DataFrame, top_k: int = 10) -> List[Dict]:
        """T√¨m ki·∫øm ng·ªØ nghƒ©a v·ªõi h·ªó tr·ª£ ti·∫øng Vi·ªát"""
        if not isinstance(recipe_df, pd.DataFrame) or recipe_df.empty:
            print("L·ªói: DataFrame c√¥ng th·ª©c kh√¥ng h·ª£p l·ªá ho·∫∑c r·ªóng")
            return []
        
        # Chu·∫©n b·ªã text t·ª´ recipes
        recipe_texts = []
        for _, recipe in recipe_df.iterrows():
            text_parts = []
            for col in ['name', 'ingredients', 'tags', 'description']:
                if col in recipe and pd.notna(recipe[col]):
                    original_text = str(recipe[col])
                    normalized_text = self.normalize_vietnamese_text(original_text)
                    text_parts.extend([original_text, normalized_text])
            recipe_texts.append(' '.join(text_parts))
        
        # Chu·∫©n b·ªã query
        normalized_query = self.normalize_vietnamese_text(query)
        combined_query = f"{query} {normalized_query}"
        
        # T·∫°o vectorizer v·ªõi stop words ti·∫øng Vi·ªát
        all_stopwords = list(self.vietnamese_stopwords) + ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']
        
        vectorizer = TfidfVectorizer(
            max_features=8000,
            stop_words=all_stopwords,
            ngram_range=(1, 3),
            lowercase=True,
            min_df=1,
            max_df=0.8,
            token_pattern=r'[a-zA-Z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒëƒê]+'
        )
        
        try:
            all_texts = recipe_texts + [combined_query]
            tfidf_matrix = vectorizer.fit_transform(all_texts)
            query_vector = tfidf_matrix[-1]
            recipe_vectors = tfidf_matrix[:-1]
            similarities = cosine_similarity(query_vector, recipe_vectors).flatten()
            
            # C·∫£i thi·ªán scoring v·ªõi weight cho c√°c y·∫øu t·ªë quan tr·ªçng
            enhanced_scores = []
            intent = self.extract_intent(query)
            
            for idx, base_score in enumerate(similarities):
                recipe = recipe_df.iloc[idx]
                enhanced_score = base_score
                
                # Bonus cho exact name match
                recipe_name = str(recipe.get('name', '')).lower()
                if any(word in recipe_name for word in query.lower().split()):
                    enhanced_score += 0.3
                
                # Bonus cho cuisine match
                if intent['cuisine']:
                    recipe_text = ' '.join([
                        str(recipe.get('name', '')),
                        str(recipe.get('tags', '')),
                        str(recipe.get('ingredients', ''))
                    ]).lower()
                    
                    for cuisine in intent['cuisine']:
                        if cuisine in recipe_text or any(kw in recipe_text for kw in self.cuisine_keywords.get(cuisine, [])):
                            enhanced_score += 0.2
                            break
                
                # Bonus cho ingredient match
                if intent['ingredients']:
                    recipe_ingredients = str(recipe.get('ingredients', '')).lower()
                    for ingredient in intent['ingredients']:
                        if ingredient in recipe_ingredients or any(kw in recipe_ingredients for kw in self.ingredient_keywords.get(ingredient, [])):
                            enhanced_score += 0.15
                
                enhanced_scores.append(enhanced_score)
            
            # S·∫Øp x·∫øp theo enhanced score
            enhanced_scores = np.array(enhanced_scores)
            top_indices = enhanced_scores.argsort()[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if enhanced_scores[idx] > 0.05:  # Threshold th·∫•p h∆°n ƒë·ªÉ c√≥ nhi·ªÅu k·∫øt qu·∫£ h∆°n
                    recipe = recipe_df.iloc[idx]
                    results.append({
                        'recipe_id': recipe.get('id', idx),
                        'name': recipe.get('name', ''),
                        'score': float(enhanced_scores[idx]),
                        'base_score': float(similarities[idx]),
                        'ingredients': recipe.get('ingredients', ''),
                        'tags': recipe.get('tags', ''),
                        'description': recipe.get('description', ''),
                        'cuisine': recipe.get('cuisine', ''),
                        'nutrition': recipe.get('nutrition', [0]),
                        'cooking_time': recipe.get('cooking_time', ''),
                        'difficulty': recipe.get('difficulty', '')
                    })
            
            return results
        except Exception as e:
            print(f"L·ªói khi th·ª±c hi·ªán t√¨m ki·∫øm ng·ªØ nghƒ©a: {e}")
            return []

    def fuzzy_match_dishes(self, query: str, dish_names: List[str], threshold: int = 60) -> List[Tuple[str, int]]:
        """T√¨m ki·∫øm m·ªù cho t√™n m√≥n ƒÉn v·ªõi h·ªó tr·ª£ ti·∫øng Vi·ªát"""
        if not dish_names:
            print("L·ªói: Danh s√°ch t√™n m√≥n ƒÉn r·ªóng")
            return []
        
        try:
            # Chu·∫©n b·ªã query variants
            original_query = query.lower()
            normalized_query = self.normalize_vietnamese_text(query)
            
            # Chu·∫©n b·ªã dish name variants
            dish_variants = {}
            for dish in dish_names:
                original_dish = dish.lower()
                normalized_dish = self.normalize_vietnamese_text(dish)
                dish_variants[original_dish] = dish
                if normalized_dish != original_dish:
                    dish_variants[normalized_dish] = dish
            
            all_dish_names = list(dish_variants.keys())
            
            # Th·ª±c hi·ªán fuzzy matching v·ªõi c√°c query variants
            matches_original = process.extract(original_query, all_dish_names, limit=15, scorer=fuzz.token_sort_ratio)
            matches_normalized = process.extract(normalized_query, all_dish_names, limit=15, scorer=fuzz.token_sort_ratio)
            
            # K·∫øt h·ª£p v√† lo·∫°i b·ªè tr√πng l·∫∑p
            all_matches = {}
            for match, score in matches_original + matches_normalized:
                original_dish_name = dish_variants[match]
                if original_dish_name not in all_matches or all_matches[original_dish_name] < score:
                    all_matches[original_dish_name] = score
            
            # S·∫Øp x·∫øp v√† l·ªçc theo threshold
            sorted_matches = sorted(all_matches.items(), key=lambda x: x[1], reverse=True)
            return [(dish, score) for dish, score in sorted_matches if score >= threshold]
            
        except Exception as e:
            print(f"L·ªói khi th·ª±c hi·ªán t√¨m ki·∫øm m·ªù: {e}")
            return []

    def extract_cooking_preferences(self, user_input: str) -> Dict:
        """Tr√≠ch xu·∫•t s·ªü th√≠ch n·∫•u ƒÉn chi ti·∫øt"""
        intent = self.extract_intent(user_input)
        
        preferences = {
            'difficulty_level': 'any',  # easy, medium, hard, any
            'cooking_time': 'any',      # quick (<30min), medium (30-60min), long (>60min), any  
            'serving_size': 'any',      # 1-2, 3-4, 5+, any
            'budget': 'any',            # low, medium, high, any
            'equipment': [],            # oven, stove, microwave, etc.
            'dietary_restrictions': intent['dietary'],
            'preferred_cuisines': intent['cuisine'],
            'disliked_ingredients': [],
            'cooking_skills': 'beginner' # beginner, intermediate, advanced
        }
        
        # Ph√¢n t√≠ch th·ªùi gian n·∫•u
        time_indicators = {
            'quick': ['nhanh', 'quick', 'fast', '15 ph√∫t', '20 ph√∫t', 't·ªëc h√†nh', 'toc hanh'],
            'medium': ['v·ª´a', 'vua', 'medium', '30 ph√∫t', '45 ph√∫t', '1 ti·∫øng', 'b√¨nh th∆∞·ªùng', 'binh thuong'],
            'long': ['l√¢u', 'lau', 'slow', 'ch·∫≠m', 'cham', '2 ti·∫øng', 'c·∫£ ng√†y', 'ca ngay']
        }
        
        normalized_input = self.normalize_vietnamese_text(user_input.lower())
        
        for time_type, keywords in time_indicators.items():
            for keyword in keywords:
                if keyword in user_input.lower() or self.normalize_vietnamese_text(keyword) in normalized_input:
                    preferences['cooking_time'] = time_type
                    break
        
        # Ph√¢n t√≠ch ƒë·ªô kh√≥
        difficulty_indicators = {
            'easy': ['d·ªÖ', 'de', 'easy', 'simple', 'ƒë∆°n gi·∫£n', 'don gian', 'c∆° b·∫£n', 'co ban'],
            'medium': ['v·ª´a', 'vua', 'medium', 'b√¨nh th∆∞·ªùng', 'binh thuong'],
            'hard': ['kh√≥', 'kho', 'hard', 'difficult', 'ph·ª©c t·∫°p', 'phuc tap', 'chuy√™n nghi·ªáp', 'chuyen nghiep']
        }
        
        for diff_type, keywords in difficulty_indicators.items():
            for keyword in keywords:
                if keyword in user_input.lower() or self.normalize_vietnamese_text(keyword) in normalized_input:
                    preferences['difficulty_level'] = diff_type
                    break
        
        return preferences

    def suggest_recipes_by_mood(self, mood: str) -> List[str]:
        """G·ª£i √Ω m√≥n ƒÉn theo t√¢m tr·∫°ng"""
        mood_to_food = {
            'happy': ['b√°nh ng·ªçt', 'kem', 'pizza', 'hamburger', 'ƒë·ªì chi√™n gi√≤n'],
            'sad': ['ch√°o', 's√∫p', 'ph·ªü', 'b√∫n ri√™u', 'ƒë·ªì ƒÉn ·∫•m n√≥ng'],
            'stressed': ['tr√†', 'ch√®', 'ƒë·ªì ng·ªçt nh·∫π', 'salad', 'ƒë·ªì thanh m√°t'],
            'energetic': ['ƒë·ªì cay', 'l·∫©u', 'n∆∞·ªõng', 'barbecue', 'ƒë·ªì c√≥ nhi·ªÅu protein'],
            'tired': ['ƒë·ªì d·ªÖ n·∫•u', 'm√¨ g√≥i', 'c∆°m h·ªôp', 'ƒë·ªì ƒë√¥ng l·∫°nh'],
            'romantic': ['ƒë·ªì √ù', 'r∆∞·ª£u vang', 'steak', 'chocolate', 'tr√°ng mi·ªáng']
        }
        
        normalized_mood = self.normalize_vietnamese_text(mood.lower())
        
        # T√¨m mood ph√π h·ª£p
        for mood_key, suggestions in mood_to_food.items():
            if mood_key in normalized_mood or any(keyword in normalized_mood for keyword in [mood_key]):
                return suggestions
        
        return ['c∆°m', 'ph·ªü', 'b√°nh m√¨']  # default suggestions

    def analyze_user_query_complexity(self, query: str) -> Dict:
        """Ph√¢n t√≠ch ƒë·ªô ph·ª©c t·∫°p c·ªßa c√¢u h·ªèi ng∆∞·ªùi d√πng"""
        intent = self.extract_intent(query)
        
        complexity_score = 0
        complexity_factors = []
        
        # ƒê·∫øm s·ªë l∆∞·ª£ng ti√™u ch√≠
        criteria_count = sum([
            len(intent['cuisine']),
            len(intent['dietary']),
            len(intent['ingredients']),
            len(intent['meal_time']),
            len(intent['taste'])
        ])
        
        if criteria_count >= 3:
            complexity_score += 2
            complexity_factors.append('multiple_criteria')
        
        # Ki·ªÉm tra t·ª´ ph·ªß ƒë·ªãnh
        negative_words = ['kh√¥ng', 'khong', 'kh√¥ng mu·ªën', 'khong muon', 'not', 'no', 'without']
        if any(word in query.lower() for word in negative_words):
            complexity_score += 1
            complexity_factors.append('negative_constraints')
        
        # Ki·ªÉm tra c√¢u h·ªèi so s√°nh
        comparison_words = ['h∆°n', 'hon', 'better', 'vs', 'so v·ªõi', 'so voi', 'thay v√¨', 'thay vi']
        if any(word in query.lower() for word in comparison_words):
            complexity_score += 1
            complexity_factors.append('comparison')
        
        # ƒê·ªô d√†i c√¢u
        word_count = len(query.split())
        if word_count > 15:
            complexity_score += 1
            complexity_factors.append('long_query')
        
        complexity_level = 'simple'
        if complexity_score >= 3:
            complexity_level = 'complex'
        elif complexity_score >= 1:
            complexity_level = 'medium'
        
        return {
            'complexity_level': complexity_level,
            'complexity_score': complexity_score,
            'factors': complexity_factors,
            'criteria_count': criteria_count,
            'word_count': word_count,
            'intent': intent
        }

if __name__ == "__main__":
    # Test NLPProcessor v·ªõi nhi·ªÅu test case ti·∫øng Vi·ªát
    try:
        nlp_processor = NLPProcessor()
        print("üéâ Kh·ªüi t·∫°o NLPProcessor th√†nh c√¥ng!\n")
        
        # Test cases ti·∫øng Vi·ªát
        test_queries = [
            "T√¥i mu·ªën ƒÉn ph·ªü b√≤ kh√¥ng cay cho b·ªØa s√°ng",
            "T√¨m m√≥n ƒÉn chay √ù v·ªõi m√¨ pasta",
            "M√≥n n∆∞·ªõng cay cho bu·ªïi t·ªëi",
            "ƒê·ªì ƒÉn nhanh √≠t calo cho ng∆∞·ªùi t·∫≠p gym",
            "B√°nh ng·ªçt ti·∫øng Ph√°p d·ªÖ l√†m",
            "L·∫©u Th√°i chua cay nhi·ªÅu rau",
            "C∆°m t·∫•m s∆∞·ªùn n∆∞·ªõng mi·ªÅn Nam",
            "M√≥n Nh·∫≠t thanh ƒë·∫°m cho ng∆∞·ªùi ƒÉn ki√™ng"
        ]
        
        print("=== TEST INTENT EXTRACTION ===")
        for i, query in enumerate(test_queries, 1):
            print(f"\n{i}. Query: '{query}'")
            intent = nlp_processor.extract_intent(query)
            print(f"   Cuisine: {intent['cuisine']}")
            print(f"   Dietary: {intent['dietary']}")
            print(f"   Ingredients: {intent['ingredients']}")
            print(f"   Taste: {intent['taste']}")
            print(f"   Meal time: {intent['meal_time']}")
            print(f"   Confidence: {intent['confidence']:.2f}")
            
            # Test complexity analysis
            complexity = nlp_processor.analyze_user_query_complexity(query)
            print(f"   Complexity: {complexity['complexity_level']} (score: {complexity['complexity_score']})")
        
        print("\n=== TEST FUZZY MATCHING ===")
        sample_dishes = [
            "Ph·ªü B√≤ T√°i", "B√°nh M√¨ Th·ªãt N∆∞·ªõng", "C∆°m T·∫•m S∆∞·ªùn", "B√∫n B√≤ Hu·∫ø",
            "B√°nh X√®o Mi·ªÅn T√¢y", "G·ªèi Cu·ªën T√¥m Thit", "Ch·∫£ C√° L√£ V·ªçng", 
            "B√∫n Ch·∫£ H√† N·ªôi", "Cao L·∫ßu H·ªôi An", "M√¨ Qu·∫£ng ƒê√† N·∫µng"
        ]
        
        fuzzy_tests = ["pho bo", "banh mi", "com tam", "bun bo hue"]
        for test_query in fuzzy_tests:
            fuzzy_results = nlp_processor.fuzzy_match_dishes(test_query, sample_dishes, threshold=50)
            print(f"\nFuzzy search cho '{test_query}':")
            for dish, score in fuzzy_results[:3]:
                print(f"  - {dish}: {score}%")
        
        print("\n=== TEST SEMANTIC SEARCH ===")
        # T·∫°o sample DataFrame v·ªõi nhi·ªÅu m√≥n ƒÉn Vi·ªát Nam
        sample_data = pd.DataFrame({
            'id': range(1, 11),
            'name': [
                "Ph·ªü B√≤ T√°i Ch√≠n", "B√°nh M√¨ Pate", "C∆°m T·∫•m S∆∞·ªùn N∆∞·ªõng", 
                "B√∫n B√≤ Hu·∫ø", "B√°nh X√®o T√¥m Th·ªãt", "G·ªèi Cu·ªën Chay",
                "Ch·∫£ C√° ThƒÉng Long", "B√∫n Ch·∫£ H√† N·ªôi", "Cao L·∫ßu H·ªôi An", "M√¨ Qu·∫£ng G√†"
            ],
            'ingredients': [
                "b√°nh ph·ªü, th·ªãt b√≤, h√†nh t√¢y, ng√≤", "b√°nh m√¨, pate, rau s·ªëng, d∆∞a chua",
                "c∆°m t·∫•m, s∆∞·ªùn n∆∞·ªõng, ch·∫£, b√¨", "b√∫n, th·ªãt b√≤, ch·∫£ cua, t√¥m",
                "b·ªôt b√°nh x√®o, t√¥m, th·ªãt, gi√° ƒë·ªó", "b√°nh tr√°ng, rau s·ªëng, b√∫n, ƒë·∫≠u h≈©",
                "c√° lƒÉng, th·ªãt n∆∞·ªõng, b√∫n, rau th∆°m", "b√∫n, th·ªãt n∆∞·ªõng, ch·∫£, rau",
                "m√¨ cao l·∫ßu, th·ªãt heo, t√¥m kh√¥", "m√¨ qu·∫£ng, g√†, t√¥m, tr·ª©ng c√∫t"
            ],
            'tags': [
                "vietnamese, soup, beef", "vietnamese, sandwich, breakfast",
                "vietnamese, rice, grilled", "vietnamese, spicy, soup",
                "vietnamese, pancake, crispy", "vietnamese, vegetarian, fresh",
                "vietnamese, fish, noodles", "vietnamese, grilled, hanoi",
                "vietnamese, noodles, hoian", "vietnamese, noodles, chicken"
            ],
            'description': [
                "M√≥n ph·ªü truy·ªÅn th·ªëng H√† N·ªôi", "B√°nh m√¨ S√†i G√≤n th∆°m ngon",
                "C∆°m t·∫•m S√†i G√≤n ƒë·∫∑c s·∫£n", "B√∫n b√≤ Hu·∫ø cay n·ªìng",
                "B√°nh x√®o mi·ªÅn T√¢y gi√≤n r·ª•m", "G·ªèi cu·ªën chay thanh m√°t",
                "Ch·∫£ c√° H√† N·ªôi truy·ªÅn th·ªëng", "B√∫n ch·∫£ Obama n·ªïi ti·∫øng",
                "Cao l·∫ßu ƒë·∫∑c s·∫£n H·ªôi An", "M√¨ Qu·∫£ng ƒë·∫≠m ƒë√† h∆∞∆°ng v·ªã"
            ],
            'cuisine': ["vietnamese"] * 10,
            'cooking_time': ["30 ph√∫t", "15 ph√∫t", "45 ph√∫t", "60 ph√∫t", "30 ph√∫t", 
                           "20 ph√∫t", "40 ph√∫t", "35 ph√∫t", "25 ph√∫t", "50 ph√∫t"],
            'difficulty': ["medium", "easy", "medium", "hard", "medium", 
                          "easy", "medium", "medium", "easy", "medium"]
        })
        
        semantic_test_queries = [
            "m√≥n ph·ªü b√≤ cho b·ªØa s√°ng",
            "ƒë·ªì ƒÉn chay nh·∫π nh√†ng", 
            "m√≥n n∆∞·ªõng th∆°m ngon",
            "b√∫n t√¥m chua cay"
        ]
        
        for query in semantic_test_queries:
            print(f"\nSemantic search cho: '{query}'")
            results = nlp_processor.semantic_search(query, sample_data, top_k=3)
            for result in results:
                print(f"  - {result['name']}: {result['score']:.3f}")
                print(f"    Ingredients: {result['ingredients'][:50]}...")
        
        print("\n=== TEST COOKING PREFERENCES ===")
        pref_queries = [
            "T√¥i mu·ªën n·∫•u m√≥n d·ªÖ v√† nhanh d∆∞·ªõi 30 ph√∫t",
            "T√¨m m√≥n kh√≥ v√† ph·ª©c t·∫°p cho 4 ng∆∞·ªùi",
            "ƒê·ªì ƒÉn chay ƒë∆°n gi·∫£n cho ng∆∞·ªùi m·ªõi h·ªçc n·∫•u"
        ]
        
        for query in pref_queries:
            prefs = nlp_processor.extract_cooking_preferences(query)
            print(f"\nPreferences cho: '{query}'")
            print(f"  - Difficulty: {prefs['difficulty_level']}")
            print(f"  - Time: {prefs['cooking_time']}")
            print(f"  - Dietary: {prefs['dietary_restrictions']}")
        
        print("\n T·∫•t c·∫£ test cases ƒë√£ ho√†n th√†nh th√†nh c√¥ng!")
        
    except Exception as e:
        print(f" L·ªói khi ki·ªÉm tra: {e}")
        import traceback
        traceback.print_exc()